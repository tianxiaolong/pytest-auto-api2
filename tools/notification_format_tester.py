#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Notification Format Tester Module

This module provides notification format testing functionality.
"""

"""
é€šçŸ¥æ ¼å¼æµ‹è¯•å·¥å…·
ç”¨äºé¢„è§ˆå’Œå¯¹æ¯”åŸå§‹æ ¼å¼ä¸å¢å¼ºæ ¼å¼çš„é€šçŸ¥æ•ˆæœ
@Author : txl
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.other_tools.allure_data.allure_report_data import AllureFileClean
from utils.notify.enhanced_notification_formatter import format_simple_notification, format_alarm_notification
from utils.notify.alert_level_manager import calculate_alert_level, format_alert_summary


class MockTestMetrics:
    """æ¨¡æ‹Ÿæµ‹è¯•æŒ‡æ ‡ç±»"""
    
    def __init__(self, scenario: str = "normal"):
        """
        åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®
        
        Args:
            scenario: æµ‹è¯•åœºæ™¯ (excellent/good/normal/poor/critical)
        """
        scenarios = {
            "excellent": {
                "total": 100, "passed": 98, "failed": 1, "broken": 0, "skipped": 1,
                "pass_rate": 98.0, "time": "120.5", "avg_response_time": 150
            },
            "good": {
                "total": 100, "passed": 85, "failed": 10, "broken": 3, "skipped": 2,
                "pass_rate": 85.0, "time": "180.3", "avg_response_time": 350
            },
            "normal": {
                "total": 100, "passed": 70, "failed": 25, "broken": 3, "skipped": 2,
                "pass_rate": 70.0, "time": "220.8", "avg_response_time": 650
            },
            "poor": {
                "total": 100, "passed": 45, "failed": 40, "broken": 10, "skipped": 5,
                "pass_rate": 45.0, "time": "300.2", "avg_response_time": 1200
            },
            "critical": {
                "total": 100, "passed": 20, "failed": 65, "broken": 15, "skipped": 0,
                "pass_rate": 20.0, "time": "450.7", "avg_response_time": 2500
            }
        }
        
        data = scenarios.get(scenario, scenarios["normal"])
        
        # è®¾ç½®å±æ€§
        self.total = data["total"]
        self.passed = data["passed"] 
        self.failed = data["failed"]
        self.broken = data["broken"]
        self.skipped = data["skipped"]
        self.pass_rate = data["pass_rate"]
        self.time = data["time"]
        
        # å¢å¼ºæ ¼å¼éœ€è¦çš„é¢å¤–å±æ€§
        self.case_count = self.total
        self.success_count = self.passed
        self.failed_count = self.failed + self.broken
        self.skipped_count = self.skipped
        self.success_rate = self.pass_rate
        self.avg_response_time = data["avg_response_time"]
        self.project_name = "pytest-auto-api2"
        self.tester_name = "txl"
        self.environment = "æµ‹è¯•ç¯å¢ƒ"


def preview_enhanced_format(scenario: str = "normal"):
    """é¢„è§ˆå¢å¼ºæ ¼å¼é€šçŸ¥"""
    print(f"\nğŸ¨ å¢å¼ºæ ¼å¼é€šçŸ¥é¢„è§ˆ - {scenario.upper()}åœºæ™¯")
    print("=" * 80)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    mock_metrics = MockTestMetrics(scenario)
    
    # ç”Ÿæˆå¢å¼ºæ ¼å¼é€šçŸ¥
    enhanced_content = format_simple_notification(mock_metrics)
    
    print(enhanced_content)
    print("\n" + "=" * 80)


def preview_legacy_format(scenario: str = "normal"):
    """é¢„è§ˆåŸå§‹æ ¼å¼é€šçŸ¥"""
    print(f"\nğŸ“ åŸå§‹æ ¼å¼é€šçŸ¥é¢„è§ˆ - {scenario.upper()}åœºæ™¯")
    print("=" * 80)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    mock_metrics = MockTestMetrics(scenario)
    
    # æ¨¡æ‹ŸåŸå§‹æ ¼å¼ï¼ˆä¼ä¸šå¾®ä¿¡ï¼‰
    legacy_content = f"""ã€{mock_metrics.project_name}è‡ªåŠ¨åŒ–é€šçŸ¥ã€‘
>æµ‹è¯•ç¯å¢ƒï¼š<font color="info">TEST</font>
>æµ‹è¯•è´Ÿè´£äººï¼š@{mock_metrics.tester_name}
>
> **æ‰§è¡Œç»“æœ**
><font color="info">æˆ  åŠŸ  ç‡  : {mock_metrics.pass_rate}%</font>
>ç”¨ä¾‹  æ€»æ•°ï¼š<font color="info">{mock_metrics.total}</font>
>æˆåŠŸç”¨ä¾‹æ•°ï¼š<font color="info">{mock_metrics.passed}</font>
>å¤±è´¥ç”¨ä¾‹æ•°ï¼š`{mock_metrics.failed}ä¸ª`
>å¼‚å¸¸ç”¨ä¾‹æ•°ï¼š`{mock_metrics.broken}ä¸ª`
>è·³è¿‡ç”¨ä¾‹æ•°ï¼š<font color="warning">{mock_metrics.skipped}ä¸ª</font>
>ç”¨ä¾‹æ‰§è¡Œæ—¶é•¿ï¼š<font color="warning">{mock_metrics.time} s</font>
>
>éç›¸å…³è´Ÿè´£äººå‘˜å¯å¿½ç•¥æ­¤æ¶ˆæ¯ã€‚
>ğŸ“Š [æµ‹è¯•æŠ¥å‘Šé“¾æ¥1](http://localhost:9999/index.html)
>ğŸ“Š [æµ‹è¯•æŠ¥å‘Šé“¾æ¥2](http://127.0.0.1:9999/index.html)
>
>ğŸ’¡ **æŠ¥å‘Šè®¿é—®è¯´æ˜**ï¼š
>- å¦‚æœé“¾æ¥1æ— æ³•è®¿é—®ï¼Œå¯ä»¥å°è¯•é“¾æ¥2æˆ–é“¾æ¥3
>- æˆ–å¤åˆ¶é“¾æ¥åˆ°æµè§ˆå™¨ä¸­æ‰“å¼€
>- æŠ¥å‘Šæ–‡ä»¶ä½ç½®ï¼š./report/html/index.html"""
    
    print(legacy_content)
    print("\n" + "=" * 80)


def compare_formats(scenario: str = "normal"):
    """å¯¹æ¯”ä¸¤ç§æ ¼å¼"""
    print(f"\nğŸ”„ æ ¼å¼å¯¹æ¯” - {scenario.upper()}åœºæ™¯")
    print("=" * 100)
    
    preview_legacy_format(scenario)
    preview_enhanced_format(scenario)
    
    # æ˜¾ç¤ºå‘Šè­¦çº§åˆ«åˆ†æ
    mock_metrics = MockTestMetrics(scenario)
    alert_info = calculate_alert_level(
        mock_metrics.success_rate, 
        mock_metrics.total, 
        mock_metrics.failed_count
    )
    
    print(f"\nğŸ“Š å‘Šè­¦çº§åˆ«åˆ†æ:")
    print(f"   {format_alert_summary(alert_info)}")
    print("=" * 100)


def test_all_scenarios():
    """æµ‹è¯•æ‰€æœ‰åœºæ™¯"""
    scenarios = ["excellent", "good", "normal", "poor", "critical"]
    
    print("ğŸ§ª é€šçŸ¥æ ¼å¼å…¨åœºæ™¯æµ‹è¯•")
    print("=" * 120)
    
    for scenario in scenarios:
        compare_formats(scenario)
        input(f"\næŒ‰å›è½¦é”®ç»§ç»­æŸ¥çœ‹ä¸‹ä¸€ä¸ªåœºæ™¯...")


def show_alert_level_demo():
    """å±•ç¤ºå‘Šè­¦çº§åˆ«æ¼”ç¤º"""
    print("\nğŸš¨ å‘Šè­¦çº§åˆ«æ¼”ç¤º")
    print("=" * 60)
    
    scenarios = {
        "excellent": "ä¼˜ç§€åœºæ™¯",
        "good": "è‰¯å¥½åœºæ™¯", 
        "normal": "ä¸€èˆ¬åœºæ™¯",
        "poor": "è¾ƒå·®åœºæ™¯",
        "critical": "ä¸¥é‡åœºæ™¯"
    }
    
    for scenario, desc in scenarios.items():
        mock_metrics = MockTestMetrics(scenario)
        alert_info = calculate_alert_level(
            mock_metrics.success_rate,
            mock_metrics.total, 
            mock_metrics.failed_count
        )
        
        summary = format_alert_summary(alert_info)
        print(f"{desc:8} -> {summary}")
    
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¨ é€šçŸ¥æ ¼å¼æµ‹è¯•å·¥å…·")
    print("=" * 80)
    
    while True:
        print("\nè¯·é€‰æ‹©è¦æ‰§è¡Œçš„æ“ä½œ:")
        print("1. ğŸ“± é¢„è§ˆå¢å¼ºæ ¼å¼é€šçŸ¥")
        print("2. ğŸ“ é¢„è§ˆåŸå§‹æ ¼å¼é€šçŸ¥") 
        print("3. ğŸ”„ å¯¹æ¯”ä¸¤ç§æ ¼å¼")
        print("4. ğŸ§ª æµ‹è¯•æ‰€æœ‰åœºæ™¯")
        print("5. ğŸš¨ å‘Šè­¦çº§åˆ«æ¼”ç¤º")
        print("6. ğŸ¯ è‡ªå®šä¹‰åœºæ™¯æµ‹è¯•")
        print("0. é€€å‡º")
        
        try:
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-6): ").strip()
            
            if choice == '0':
                print("\nğŸ‘‹ å†è§ï¼")
                break
            elif choice == '1':
                scenario = input("è¯·è¾“å…¥åœºæ™¯ (excellent/good/normal/poor/critical) [é»˜è®¤:normal]: ").strip() or "normal"
                preview_enhanced_format(scenario)
            elif choice == '2':
                scenario = input("è¯·è¾“å…¥åœºæ™¯ (excellent/good/normal/poor/critical) [é»˜è®¤:normal]: ").strip() or "normal"
                preview_legacy_format(scenario)
            elif choice == '3':
                scenario = input("è¯·è¾“å…¥åœºæ™¯ (excellent/good/normal/poor/critical) [é»˜è®¤:normal]: ").strip() or "normal"
                compare_formats(scenario)
            elif choice == '4':
                test_all_scenarios()
            elif choice == '5':
                show_alert_level_demo()
            elif choice == '6':
                print("\nè‡ªå®šä¹‰åœºæ™¯æµ‹è¯•:")
                total = int(input("æ€»ç”¨ä¾‹æ•°: ") or "100")
                passed = int(input("æˆåŠŸç”¨ä¾‹æ•°: ") or "70")
                failed = int(input("å¤±è´¥ç”¨ä¾‹æ•°: ") or "25")
                broken = int(input("å¼‚å¸¸ç”¨ä¾‹æ•°: ") or "3")
                skipped = int(input("è·³è¿‡ç”¨ä¾‹æ•°: ") or "2")
                
                # åˆ›å»ºè‡ªå®šä¹‰åœºæ™¯
                custom_metrics = MockTestMetrics()
                custom_metrics.total = total
                custom_metrics.passed = passed
                custom_metrics.failed = failed
                custom_metrics.broken = broken
                custom_metrics.skipped = skipped
                custom_metrics.pass_rate = (passed / total * 100) if total > 0 else 0
                custom_metrics.success_rate = custom_metrics.pass_rate
                custom_metrics.failed_count = failed + broken
                
                enhanced_content = format_simple_notification(custom_metrics)
                print("\nğŸ¨ è‡ªå®šä¹‰åœºæ™¯å¢å¼ºæ ¼å¼é¢„è§ˆ:")
                print("=" * 80)
                print(enhanced_content)
            else:
                print("\nâŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
                
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            input("æŒ‰å›è½¦é”®ç»§ç»­...")


if __name__ == "__main__":
    main()
